# c:\Users\Ayan Babwany\rawrr\deepresachcode\geminicode-adpatedwithgroq\deeepsearchv1\config.py

```python
import os
from dotenv import load_dotenv

load_dotenv()

class Config:
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
    TOR_PROXY = os.getenv("TOR_PROXY", "127.0.0.1:9050")
    TAVILY_API_KEY = os.getenv("TAVILY_API_KEY", None)  # Optional clearnet fallback
    CHROMA_DB_PATH = "chroma_db"
    CRAWL_DEPTH = 10
    CRAWL_DELAY_MIN = 2
    CRAWL_DELAY_MAX = 5
    LINK_LIMIT_PER_PAGE = 5
    GEMINI_MODELS = ["gemini-2.0-flash-exp", "gemini-2.5-pro-preview-05-06"]
    DEFAULT_MODEL = "gemini-2.5-pro-preview-05-06"
    TEMPERATURE = 0.7
    MAX_TOKENS = 1000000
```

# c:\Users\Ayan Babwany\rawrr\deepresachcode\geminicode-adpatedwithgroq\deeepsearchv1\knowledge_base.py

```python
from langchain_community.vectorstores import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from config import Config
from utils import log_action

class KnowledgeBase:
    def __init__(self):
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001",
            google_api_key=Config.GEMINI_API_KEY
        )
        self.vectordb = Chroma(
            persist_directory=Config.CHROMA_DB_PATH,
            embedding_function=self.embeddings
        )

    def store_data(self, data: list):
        texts = [d["content"] for d in data if d["content"]]
        metadatas = [{"url": d["url"]} for d in data if d["content"]]
        if texts:
            log_action(f"Storing {len(texts)} documents in knowledge base")
            self.vectordb.add_texts(texts, metadatas=metadatas)
            self.vectordb.persist()

    def retrieve_data(self, query: str, k=5):
        log_action(f"Retrieving data for query: {query}")
        return self.vectordb.similarity_search(query, k=k)
```

# c:\Users\Ayan Babwany\rawrr\deepresachcode\geminicode-adpatedwithgroq\deeepsearchv1\utils.py

```python
import logging
from rich.console import Console
import random
import time

console = Console()

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    filename="research_agent.log"
)

def log_action(message):
    logging.info(message)
    console.print(f"[bold blue]{message}[/bold blue]")

def randomize_delay(min_delay=2, max_delay=5):
    delay = random.uniform(min_delay, max_delay)
    time.sleep(delay)
    return delay
```

# c:\Users\Ayan Babwany\rawrr\deepresachcode\geminicode-adpatedwithgroq\deeepsearchv1\agent.py

```python
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.tools import BaseTool
from langchain.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph
from typing import TypedDict, List
from config import Config
from crawler import TorCrawler
from knowledge_base import KnowledgeBase
from utils import log_action

class AgentState(TypedDict):
    query: str
    plan: List[str]
    crawled_data: List[dict]
    report: str
    chat_history: List[dict]

class TorSearchTool(BaseTool):
    name: str = "tor_search"  # Added type annotation
    description: str = "Crawls onion sites for dark web content."  # Added type annotation
    
    def _run(self, query: str):
        crawler = TorCrawler()
        # Replace with dynamic onion site discovery (e.g., Hidden Wiki, Dread)
        onion_url = "http://dread.onion"  # Placeholder
        data = crawler.crawl_onion(onion_url)
        crawler.close()
        return data

class ResearchAgent:
    def __init__(self, model_name=Config.DEFAULT_MODEL):
        self.llm = ChatGoogleGenerativeAI(
            model=model_name,
            google_api_key=Config.GEMINI_API_KEY,
            temperature=Config.TEMPERATURE,
            max_output_tokens=Config.MAX_TOKENS
        )
        self.kb = KnowledgeBase()
        self.tools = [TorSearchTool()]
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a deep research agent specializing in dark web analysis. Break down queries into sub-tasks, use tools to gather data, and generate detailed markdown reports. Avoid illegal activities and focus on ethical research."),
            ("human", "{query}")
        ])

    def planner_node(self, state: AgentState):
        log_action("Generating research plan")
        prompt = self.prompt.format(query=state["query"])
        plan = self.llm.invoke(prompt).content.split("\n")
        return {"plan": [p.strip() for p in plan if p.strip()]}

    def crawler_node(self, state: AgentState):
        log_action("Executing crawler")
        data = []
        for sub_task in state["plan"]:
            for tool in self.tools:
                result = tool._run(sub_task)
                data.append(result)
        self.kb.store_data(data)
        return {"crawled_data": data}

    def analyzer_node(self, state: AgentState):
        log_action("Analyzing crawled data")
        context = self.kb.retrieve_data(state["query"])
        prompt = f"Analyze dark web data:\n{state['crawled_data']}\n\nContext from knowledge base:\n{context}"
        analysis = self.llm.invoke(prompt).content
        return {"report": analysis}

    def report_node(self, state: AgentState):
        log_action("Generating final report")
        prompt = f"Generate a detailed markdown report from:\n{state['report']}"
        report = self.llm.invoke(prompt).content
        return {"report": report}

    def build_graph(self):
        graph = StateGraph(AgentState)
        graph.add_node("planner", self.planner_node)
        graph.add_node("crawler", self.crawler_node)
        graph.add_node("analyzer", self.analyzer_node)
        graph.add_node("report_generator", self.report_node)
        graph.add_edge("planner", "crawler")
        graph.add_edge("crawler", "analyzer")
        graph.add_edge("analyzer", "report_generator")
        graph.set_entry_point("planner")
        return graph.compile()
```

# c:\Users\Ayan Babwany\rawrr\deepresachcode\geminicode-adpatedwithgroq\deeepsearchv1\crawler.py

```python
import requests
from bs4 import BeautifulSoup
import random
from config import Config
from utils import log_action, randomize_delay

class TorCrawler:
    def __init__(self):
        self.proxy = Config.TOR_PROXY
        self.session = None
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Safari/605.1.15",
            # Add more user agents
        ]

    def start_tor_session(self):
        log_action("Starting Tor session via SOCKS proxy")
        self.session = requests.Session()
        self.session.proxies = {"http": f"socks5://{self.proxy}", "https": f"socks5://{self.proxy}"}
        # Check if Tor proxy is reachable
        connection_status = self.check_tor_connection()
        if connection_status:
            log_action("Tor proxy connection successful")
        else:
            log_action("Warning: Tor proxy connection failed. Crawling may not work as expected.")

    def check_tor_connection(self):
        """
        Check if the Tor proxy is reachable by attempting a simple request.
        Returns True if connection is successful, False otherwise.
        """
        log_action("Checking Tor proxy connection...")
        if not self.session:
            log_action("Session not initialized. Cannot check Tor connection.")
            return False
        
        test_url = "https://check.torproject.org/api/ip"
        headers = {"User-Agent": random.choice(self.user_agents)}
        try:
            response = self.session.get(test_url, headers=headers, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if data.get("IsTor", False):
                    return True
                else:
                    log_action("Tor proxy connected but not routing through Tor network.")
                    return False
            else:
                log_action(f"Unexpected response code from Tor check: {response.status_code}")
                return False
        except requests.exceptions.RequestException as e:
            log_action(f"Failed to connect to Tor proxy due to request error: {str(e)}")
            return False
        except Exception as e:
            log_action(f"Failed to connect to Tor proxy due to unexpected error: {str(e)}")
            return False

    def crawl_onion(self, url, max_depth=Config.CRAWL_DEPTH):
        if not self.session:
            self.start_tor_session()
        
        headers = {"User-Agent": random.choice(self.user_agents)}
        crawled_data = {"url": url, "content": "", "links": [], "errors": []}
        
        try:
            log_action(f"Crawling: {url}")
            response = self.session.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            
            # Extract text and links
            crawled_data["content"] = soup.get_text(separator=" ", strip=True)
            crawled_data["links"] = [a["href"] for a in soup.find_all("a", href=True) if ".onion" in a["href"]]
            
            # Recursive crawling
            if max_depth > 0:
                for link in crawled_data["links"][:Config.LINK_LIMIT_PER_PAGE]:
                    randomize_delay(Config.CRAWL_DELAY_MIN, Config.CRAWL_DELAY_MAX)
                    sub_data = self.crawl_onion(link, max_depth - 1)
                    crawled_data["content"] += f"\n\n--- Subpage: {link} ---\n{sub_data['content']}"
                    crawled_data["links"].extend(sub_data["links"])
                    crawled_data["errors"].extend(sub_data["errors"])
            
        except Exception as e:
            error_msg = f"Error crawling {url}: {str(e)}"
            log_action(error_msg)
            crawled_data["errors"].append(error_msg)
        
        return crawled_data

    def close(self):
        if self.session:
            log_action("Closing Tor session")
            self.session.close()
```

# c:\Users\Ayan Babwany\rawrr\deepresachcode\geminicode-adpatedwithgroq\deeepsearchv1\app.py

```python
import streamlit as st
from config import Config
from agent import ResearchAgent
from utils import log_action
import networkx as nx
import matplotlib.pyplot as plt
import io
import base64

st.set_page_config(page_title="Tor Deep Research Agent", layout="wide")
st.title("🕵️‍♀️ Tor Web Deep Research Agent")
st.markdown("Unleash a powerful AI to explore the dark web’s depths. Configure your research, select your model, and dive in.")

# Sidebar for configuration
with st.sidebar:
    st.header("Configuration")
    model_choice = st.selectbox("Gemini Model", Config.GEMINI_MODELS, index=Config.GEMINI_MODELS.index(Config.DEFAULT_MODEL))
    depth = st.slider("Crawl Depth", 1, 50, Config.CRAWL_DEPTH)
    tor_enabled = st.checkbox("Enable Tor Network", value=True)
    # Check Tor connection status if enabled, using session state to avoid repeated checks
    if tor_enabled:
        if "tor_status" not in st.session_state:
            from crawler import TorCrawler
            crawler = TorCrawler()
            crawler.start_tor_session()  # Initialize session before checking connection
            st.session_state["tor_status"] = crawler.check_tor_connection()
            crawler.close()
        status_color = "green" if st.session_state["tor_status"] else "red"
        status_text = "Connected" if st.session_state["tor_status"] else "Disconnected"
        st.markdown(f"<span style='color: {status_color}; font-weight: bold;'>Tor Status: {status_text} ●</span>", unsafe_allow_html=True)
    else:
        st.session_state.pop("tor_status", None)  # Clear status if disabled
        st.markdown("<span style='color: grey; font-weight: bold;'>Tor Status: Disabled ●</span>", unsafe_allow_html=True)
    mode = st.selectbox("Research Mode", ["Exploratory", "Deep Dive", "Stealth"])

# Main input
query = st.text_area("Research Query", placeholder="E.g., Investigate dark web forums for cryptocurrency scams")
if st.button("Start Research"):
    st.session_state["research_query"] = query
    st.session_state["model_choice"] = model_choice
    st.session_state["depth"] = depth
    st.session_state["tor_enabled"] = tor_enabled
    st.session_state["mode"] = mode
    st.write("Research in progress...")

# Run research
if "research_query" in st.session_state:
    agent = ResearchAgent(model_name=st.session_state["model_choice"])
    graph = agent.build_graph()
    
    inputs = {
        "query": st.session_state["research_query"],
        "plan": [],
        "crawled_data": [],
        "report": "",
        "chat_history": []
    }
    
    st.subheader("Research Progress")
    progress = st.empty()
    
    results = []
    for output in graph.stream(inputs):
        results.append(output)
        with progress.container():
            st.json(output)
    
    final_report = results[-1]["report"]["report"]
    st.subheader("Final Report")
    st.markdown(final_report)
        
    # Visualize link network
    st.subheader("Link Network Visualization")
    G = nx.DiGraph()
    for data in results[-2]["crawler"]["crawled_data"]:
        for link in data["links"]:
            G.add_edge(data["url"], link)
    
    plt.figure(figsize=(10, 8))
    nx.draw(G, with_labels=True, node_color="lightblue", node_size=500, font_size=8)
    buf = io.BytesIO()
    plt.savefig(buf, format="png")
    buf.seek(0)
    img_str = base64.b64encode(buf.read()).decode()
    st.image(f"data:image/png;base64,{img_str}")
```

